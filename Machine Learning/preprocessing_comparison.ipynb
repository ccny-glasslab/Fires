{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "descending-traffic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sharedData2/nshakoor/.conda/envs/fires/lib/python3.7/site-packages/pyresample/bilinear/__init__.py:50: UserWarning: XArray and/or zarr not found, XArrayBilinearResampler won't be available.\n",
      "  warnings.warn(\"XArray and/or zarr not found, XArrayBilinearResampler won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "random.seed(42)\n",
    "\n",
    "import metpy\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from PIL import Image\n",
    "from pyresample import geometry, grid\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = hub.load(\"https://tfhub.dev/captain-pool/esrgan-tf2/1\")\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "progressive-player",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../GOES_Files/npy_files/'\n",
    "image = np.load(path + random.choice(os.listdir(path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-northwest",
   "metadata": {},
   "source": [
    "## 1 Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "french-taxation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.60175735\n",
      "0.8980085\n"
     ]
    }
   ],
   "source": [
    "height, width = image.shape\n",
    "smallimg = resize(image, (round(height/2), round(width/2)))\n",
    "control = resize(smallimg, (height, width))\n",
    "mae = mean_absolute_error(image, control)\n",
    "rmse = mean_squared_error(image, control, squared=False)\n",
    "print(mae)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-proportion",
   "metadata": {},
   "source": [
    "## Multiple Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "assisted-horizon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(array):\n",
    "    \"\"\"\n",
    "    Returns loaded .npy file.\n",
    "    \n",
    "    Parameter path: Path to load .npy file from\n",
    "    Precondition: path is a string\n",
    "    \"\"\"\n",
    "    result = np.zeros((array.shape[0], array.shape[1], 3))\n",
    "    result[:,:,0]= array\n",
    "    result[:,:,1]= array\n",
    "    result[:,:,2]= array\n",
    "#     result = result.astype('uint8')\n",
    "    return result\n",
    "\n",
    "def preprocess_image(array):\n",
    "    \"\"\"\n",
    "    Returns preprocessed input array.\n",
    "    \n",
    "    Parameter array: array to preprocess\n",
    "    Precondition: array is a numpy array\n",
    "    \"\"\"\n",
    "    hr_image = array\n",
    "    hr_size = (tf.convert_to_tensor(hr_image.shape[:-1]) // 2) * 2\n",
    "    hr_image = tf.image.crop_to_bounding_box(hr_image, 0, 0, hr_size[0], hr_size[1])\n",
    "    hr_image = tf.cast(hr_image, tf.float32)\n",
    "    return tf.expand_dims(hr_image, 0)\n",
    "\n",
    "def downscale_image(image):\n",
    "    \"\"\"\n",
    "    Returns low resolution image after scaling down input image using nearest neighbor downsampling.\n",
    "\n",
    "    Parameter image: 3D of 4D tensor of preprocessed image\n",
    "    Precondition: image is a tensor\n",
    "    \"\"\"\n",
    "#     image_size = []\n",
    "#     if len(image.shape) == 3:\n",
    "#         image_size = [image.shape[1], image.shape[0]]\n",
    "#     else:\n",
    "#         raise ValueError(\"Dimension mismatch. Can work only on single image.\")\n",
    "\n",
    "# #     image = tf.squeeze(tf.cast(image, tf.uint8))\n",
    "#     lr_image = np.asarray(Image.fromarray(image.numpy()).resize([image_size[0] // 2, image_size[1] // 2]))\n",
    "\n",
    "# #     lr_image = tf.expand_dims(lr_image, 0)\n",
    "# #     lr_image = tf.cast(lr_image, tf.float32)\n",
    "#     return image\n",
    "    height, width = image.shape\n",
    "    result = np.zeros((image.shape[0]//2, image.shape[1]//2, 3))\n",
    "    smallimg = resize(image, (round(height/2), round(width/2)))\n",
    "    result[:,:,0] = smallimg\n",
    "    result[:,:,1] = smallimg\n",
    "    result[:,:,2] = smallimg\n",
    "    lr_image = tf.expand_dims(result, 0)\n",
    "    lr_image = tf.cast(lr_image, tf.float32)\n",
    "    return lr_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "bound-decimal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.60175735\n",
      "1.5496775\n",
      "1.9424095\n"
     ]
    }
   ],
   "source": [
    "hr_image = preprocess_image(load_image(image))\n",
    "lr_image = downscale_image(image)\n",
    "fake_image = model(lr_image)\n",
    "fake_image = tf.squeeze(fake_image)\n",
    "\n",
    "hr_image = tf.squeeze(hr_image).numpy()\n",
    "lr_image = tf.squeeze(lr_image).numpy()\n",
    "lr_image = resize(lr_image, (hr_image.shape[0], hr_image.shape[1], 3)).ravel()\n",
    "# lr_image[:,:,0] = resize(lr_image[:,:,0], (hr_image.shape[0], hr_image.shape[1]))\n",
    "# lr_image = tf.squeeze(lr_image).numpy().resize([hr_image.shape[0], hr_image.shape[1]])[:,:,0]\n",
    "# # lr_image = np.asarray(Image.fromarray(tf.squeeze(lr_image).numpy()).resize([hr_image.shape[0], hr_image.shape[1]]))[:,:,0]\n",
    "fake_image = resize(fake_image.numpy(), (hr_image.shape[0], hr_image.shape[1], 3)).ravel()\n",
    "hr_image = hr_image.ravel()\n",
    "\n",
    "control_mae = mean_absolute_error(hr_image, lr_image)\n",
    "control_rmse = mean_squared_error(hr_image, lr_image, squared=False)\n",
    "\n",
    "mae = mean_absolute_error(hr_image, fake_image)\n",
    "rmse = mean_squared_error(hr_image, fake_image, squared=False)\n",
    "print(control_mae)\n",
    "print(mae)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "romantic-sacrifice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 500, 3)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "composite-celebrity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 500, 3)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "documentary-pantyhose",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 500, 3)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "preliminary-mechanics",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-d9269393a6d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlr_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "lr_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fatty-match",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 500, 500, 3])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "opposed-announcement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1000, 1000), dtype=float32, numpy=\n",
       "array([[169.73962, 239.87773, 276.17017, ..., 268.43512, 257.08893,\n",
       "        249.33235],\n",
       "       [249.64377, 268.67477, 269.13455, ..., 268.24387, 283.46872,\n",
       "        261.65567],\n",
       "       [274.6035 , 288.28812, 273.34018, ..., 261.06485, 275.35196,\n",
       "        252.22685],\n",
       "       ...,\n",
       "       [267.17377, 282.70065, 279.44284, ..., 285.3578 , 279.8649 ,\n",
       "        287.02298],\n",
       "       [266.06888, 279.07535, 261.0966 , ..., 293.8519 , 291.835  ,\n",
       "        288.44467],\n",
       "       [259.67664, 272.0229 , 263.03055, ..., 296.18527, 292.31326,\n",
       "        280.96405]], dtype=float32)>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_image[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-portland",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "x = ax[0].imshow(, cmap='hot', aspect='equal')\n",
    "ax[0].set_title('HF = HR – LR')\n",
    "ax[0].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "secondary-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "pic = fake_image[0,:,:,0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "headed-pride",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([500, 500, 3])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "narrative-screen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[240, 240, 241, ...,   4,   4,   4],\n",
       "       [240, 240, 241, ...,   4,   4,   4],\n",
       "       [239, 240, 242, ...,   4,   4,   4],\n",
       "       ...,\n",
       "       [ 31,  31,  30, ...,   6,   5,   5],\n",
       "       [ 30,  30,  30, ...,   4,   3,   3],\n",
       "       [ 30,  30,  30, ...,   3,   2,   2]], dtype=uint8)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_image[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "material-amazon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1000, 1000), dtype=float32, numpy=\n",
       "array([[147.21501  , 208.58575  , 241.5658   , ...,   3.406515 ,\n",
       "          2.948714 ,   3.010914 ],\n",
       "       [215.34886  , 230.84985  , 231.39037  , ...,   3.030803 ,\n",
       "          2.9688818,   2.860043 ],\n",
       "       [236.14165  , 246.19382  , 234.54842  , ...,   3.09882  ,\n",
       "          2.9988825,   2.8757493],\n",
       "       ...,\n",
       "       [ 28.99264  ,  30.568092 ,  30.410713 , ...,   1.1022397,\n",
       "          1.6600069,   1.7966036],\n",
       "       [ 29.3202   ,  30.831007 ,  28.597977 , ...,  -2.5142295,\n",
       "         -1.0270957,   0.5452343],\n",
       "       [ 29.335653 ,  30.954165 ,  29.363674 , ...,  -2.5281239,\n",
       "         -1.6440666,   1.4088746]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_image[0,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eight-hydrogen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[245.86417, 240.95888, 240.95888, ..., 260.64215, 260.64215,\n",
       "        262.04288],\n",
       "       [237.21982, 239.18741, 243.07973, ..., 260.2246 , 260.84793,\n",
       "        260.84793],\n",
       "       [237.21982, 239.18741, 243.07973, ..., 260.2246 , 260.84793,\n",
       "        259.79868],\n",
       "       ...,\n",
       "       [286.79688, 286.65906, 286.65906, ..., 261.0518 , 261.6519 ,\n",
       "        260.2246 ],\n",
       "       [286.451  , 286.93393, 287.4753 , ..., 259.5825 , 259.5825 ,\n",
       "        258.00385],\n",
       "       [286.79688, 286.38123, 286.3113 , ..., 259.5825 , 258.00385,\n",
       "        258.23672]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-court",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
